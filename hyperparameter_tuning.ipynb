{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparameter_tuning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfogarty/Resume/blob/master/hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDujGFixTPuK",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "From [Hyperparameter Tuning](https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624) by [Tara Boyle](https://taraboyle.me/data-science/) in [towardsdatascience.com](https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624)\n",
        "\n",
        "Updated by [John Fogarty](https://github.com/jfogarty) for Python 3.6 and [Base2 MLI](https://github.com/base2solutions/mli) and [colab](https://colab.research.google.com) standalone evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnZkt8ymUQIm",
        "colab_type": "text"
      },
      "source": [
        "[Kaggle’s](https://www.kaggle.com/c/dont-overfit-ii) Don’t Overfit II competition presents an interesting problem. We have 20,000 rows of continuous variables, with only 250 of them belonging to the training set.\n",
        "The challenge is not to overfit.\n",
        "\n",
        "With such a small dataset — and even smaller training set, this can be a difficult task!\n",
        "In this article, we’ll explore hyperparameter optimization as a means of preventing overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPxUadYaUy4I",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "[Wikipedia states](https://en.wikipedia.org/wiki/Hyperparameter_optimization) that “hyperparameter tuning is choosing a set of optimal hyperparameters for a learning algorithm”. So what is a [hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))?\n",
        "\n",
        "> *A hyperparameter is a parameter whose value is set before the learning process begins.\n",
        "Some examples of hyperparameters include penalty in logistic regression and loss in stochastic gradient descent.*\n",
        "\n",
        "In [sklearn](https://scikit-learn.org/stable/modules/grid_search.html#grid-search), hyperparameters are passed in as arguments to the constructor of the model classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDUPmbyrVTPT",
        "colab_type": "text"
      },
      "source": [
        "## Tuning Strategies\n",
        "\n",
        "We will explore two different methods for optimizing hyperparameters:\n",
        "\n",
        "- **Grid Search**\n",
        "\n",
        "- **Random Search**\n",
        "\n",
        "We’ll begin by preparing the data and trying several different models with their default hyperparameters. From these we’ll select the top two performing methods for hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwLdIxGNVp77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(27)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnPtpiAFVTcR",
        "colab_type": "text"
      },
      "source": [
        "import os\n",
        "REPODATA='https://github.com/plotly/datasets/blob/master/titanic.csv'\n",
        "RAWDATA='https://raw.githubusercontent.com/plotly/datasets/master/titanic.csv'\n",
        "filename='titanic.csv'\n",
        "TMPDATA='./tmpData'\n",
        "if not os.path.exists(TMPDATA) : os.makedirs(TMPDATA)\n",
        "datafile=os.path.join(TMPDATA, filename)\n",
        "!curl $RAWDATA -o $datafile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDS4esRhkAZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND-b8sKfi4hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "def googledrive_load(file_id, tofile):\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    with open(tofile, 'wb') as fout:\n",
        "      downloader = MediaIoBaseDownload(fout, request)\n",
        "      done = False\n",
        "      while done is False:\n",
        "          _, done = downloader.next_chunk()\n",
        "    print(f'- Downloaded {tofile}.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7ChsJQXg2i8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "faa7fda0-8645-4024-bed2-a394670e8c0a"
      },
      "source": [
        "import os\n",
        "FILE_ID_TRAIN = '1ldAeovBgrWo3oiXlQaeuSX97xLb_r962'\n",
        "TMPDATA = './tmpData'\n",
        "trainFile = os.path.join(TMPDATA, 'train.csv')\n",
        "googledrive_load(FILE_ID_TRAIN, trainFile)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Downloaded ./tmpData/train.csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX9sYfKLmAoc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19ce6768-2e58-4f64-c32e-7b20952988d0"
      },
      "source": [
        "FILE_ID_TEST = '1YTe-lgXh8lUtz56Ftw10u-7ucoI8fW1o'\n",
        "testFile = os.path.join(TMPDATA, 'test.csv')\n",
        "googledrive_load(FILE_ID_TEST, testFile)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Downloaded ./tmpData/test.csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ffup5CEn8iA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d3d22a5-db57-452d-d16f-36137643896f"
      },
      "source": [
        "FILE_ID_SUBMISSION = '1dIxeOb6U_96Ky7Hg_cWbZeTqq7gDkybH'\n",
        "submissionFile = os.path.join(TMPDATA, 'submission.csv')\n",
        "googledrive_load(FILE_ID_SUBMISSION, submissionFile)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Downloaded ./tmpData/submission.csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj_zdQq7XaXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setting up default plotting parameters\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20.0, 7.0]\n",
        "plt.rcParams.update({'font.size': 22,})\n",
        "\n",
        "sns.set_palette('viridis')\n",
        "sns.set_style('white')\n",
        "sns.set_context('talk', font_scale=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfQhU40rWE9a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "92e6edba-138e-4ad1-b73b-3874a7a474fb"
      },
      "source": [
        "\n",
        "train = pd.read_csv(trainFile)\n",
        "test = pd.read_csv(testFile)\n",
        "\n",
        "print('Train Shape: ', train.shape)\n",
        "print('Test Shape: ', test.shape)\n",
        "\n",
        "train.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Shape:  (250, 302)\n",
            "Test Shape:  (19750, 301)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.098</td>\n",
              "      <td>2.165</td>\n",
              "      <td>0.681</td>\n",
              "      <td>-0.614</td>\n",
              "      <td>1.309</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>0.276</td>\n",
              "      <td>-2.246</td>\n",
              "      <td>1.825</td>\n",
              "      <td>-0.912</td>\n",
              "      <td>-0.107</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.826</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.177</td>\n",
              "      <td>-0.673</td>\n",
              "      <td>-0.503</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.410</td>\n",
              "      <td>-1.927</td>\n",
              "      <td>0.102</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>1.763</td>\n",
              "      <td>1.449</td>\n",
              "      <td>-1.097</td>\n",
              "      <td>-0.686</td>\n",
              "      <td>-0.250</td>\n",
              "      <td>-1.859</td>\n",
              "      <td>1.125</td>\n",
              "      <td>1.009</td>\n",
              "      <td>-2.296</td>\n",
              "      <td>0.385</td>\n",
              "      <td>-0.876</td>\n",
              "      <td>1.528</td>\n",
              "      <td>-0.144</td>\n",
              "      <td>-1.078</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.681</td>\n",
              "      <td>1.250</td>\n",
              "      <td>-0.565</td>\n",
              "      <td>-1.318</td>\n",
              "      <td>-0.923</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>2.457</td>\n",
              "      <td>0.771</td>\n",
              "      <td>-0.460</td>\n",
              "      <td>0.569</td>\n",
              "      <td>-1.320</td>\n",
              "      <td>-1.516</td>\n",
              "      <td>-2.145</td>\n",
              "      <td>-1.120</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.820</td>\n",
              "      <td>-1.049</td>\n",
              "      <td>-1.125</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.617</td>\n",
              "      <td>1.253</td>\n",
              "      <td>1.248</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-0.802</td>\n",
              "      <td>-0.896</td>\n",
              "      <td>-1.793</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>-0.601</td>\n",
              "      <td>0.569</td>\n",
              "      <td>0.867</td>\n",
              "      <td>1.347</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-0.649</td>\n",
              "      <td>0.672</td>\n",
              "      <td>-2.097</td>\n",
              "      <td>1.051</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>1.038</td>\n",
              "      <td>-1.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.081</td>\n",
              "      <td>-0.973</td>\n",
              "      <td>-0.383</td>\n",
              "      <td>0.326</td>\n",
              "      <td>-0.428</td>\n",
              "      <td>0.317</td>\n",
              "      <td>1.172</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.291</td>\n",
              "      <td>2.907</td>\n",
              "      <td>1.085</td>\n",
              "      <td>2.144</td>\n",
              "      <td>1.540</td>\n",
              "      <td>0.584</td>\n",
              "      <td>1.133</td>\n",
              "      <td>1.098</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>-0.498</td>\n",
              "      <td>0.283</td>\n",
              "      <td>-1.100</td>\n",
              "      <td>-0.417</td>\n",
              "      <td>1.382</td>\n",
              "      <td>-0.515</td>\n",
              "      <td>-1.519</td>\n",
              "      <td>0.619</td>\n",
              "      <td>-0.128</td>\n",
              "      <td>0.866</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>1.238</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.269</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>-2.721</td>\n",
              "      <td>1.659</td>\n",
              "      <td>0.106</td>\n",
              "      <td>-0.121</td>\n",
              "      <td>1.719</td>\n",
              "      <td>...</td>\n",
              "      <td>0.971</td>\n",
              "      <td>-1.489</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.917</td>\n",
              "      <td>-0.094</td>\n",
              "      <td>-1.407</td>\n",
              "      <td>0.887</td>\n",
              "      <td>-0.104</td>\n",
              "      <td>-0.583</td>\n",
              "      <td>1.267</td>\n",
              "      <td>-1.667</td>\n",
              "      <td>-2.771</td>\n",
              "      <td>-0.516</td>\n",
              "      <td>1.312</td>\n",
              "      <td>0.491</td>\n",
              "      <td>0.932</td>\n",
              "      <td>2.064</td>\n",
              "      <td>0.422</td>\n",
              "      <td>1.215</td>\n",
              "      <td>2.012</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.307</td>\n",
              "      <td>-0.059</td>\n",
              "      <td>1.121</td>\n",
              "      <td>1.333</td>\n",
              "      <td>0.211</td>\n",
              "      <td>1.753</td>\n",
              "      <td>0.053</td>\n",
              "      <td>1.274</td>\n",
              "      <td>-0.612</td>\n",
              "      <td>-0.165</td>\n",
              "      <td>-1.695</td>\n",
              "      <td>-1.257</td>\n",
              "      <td>1.359</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>-1.624</td>\n",
              "      <td>-0.458</td>\n",
              "      <td>-1.099</td>\n",
              "      <td>-0.936</td>\n",
              "      <td>0.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.523</td>\n",
              "      <td>-0.089</td>\n",
              "      <td>-0.348</td>\n",
              "      <td>0.148</td>\n",
              "      <td>-0.022</td>\n",
              "      <td>0.404</td>\n",
              "      <td>-0.023</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.459</td>\n",
              "      <td>0.478</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>0.352</td>\n",
              "      <td>1.095</td>\n",
              "      <td>0.300</td>\n",
              "      <td>-1.044</td>\n",
              "      <td>0.270</td>\n",
              "      <td>-1.038</td>\n",
              "      <td>0.144</td>\n",
              "      <td>-1.658</td>\n",
              "      <td>-0.946</td>\n",
              "      <td>0.633</td>\n",
              "      <td>-0.772</td>\n",
              "      <td>1.786</td>\n",
              "      <td>0.136</td>\n",
              "      <td>-0.103</td>\n",
              "      <td>-1.223</td>\n",
              "      <td>2.273</td>\n",
              "      <td>0.055</td>\n",
              "      <td>-2.032</td>\n",
              "      <td>-0.452</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.924</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>-0.917</td>\n",
              "      <td>1.896</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>1.074</td>\n",
              "      <td>-0.748</td>\n",
              "      <td>1.086</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>0.432</td>\n",
              "      <td>1.345</td>\n",
              "      <td>-0.491</td>\n",
              "      <td>-1.602</td>\n",
              "      <td>-0.727</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.780</td>\n",
              "      <td>-0.527</td>\n",
              "      <td>-1.122</td>\n",
              "      <td>-0.208</td>\n",
              "      <td>-0.730</td>\n",
              "      <td>-0.302</td>\n",
              "      <td>2.535</td>\n",
              "      <td>-1.045</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.020</td>\n",
              "      <td>1.373</td>\n",
              "      <td>0.456</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>1.381</td>\n",
              "      <td>1.843</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.263</td>\n",
              "      <td>-1.222</td>\n",
              "      <td>0.726</td>\n",
              "      <td>1.444</td>\n",
              "      <td>-1.165</td>\n",
              "      <td>-1.544</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.800</td>\n",
              "      <td>-1.211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>0.392</td>\n",
              "      <td>-1.637</td>\n",
              "      <td>-0.446</td>\n",
              "      <td>-0.725</td>\n",
              "      <td>-1.035</td>\n",
              "      <td>0.834</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.335</td>\n",
              "      <td>-1.148</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>1.048</td>\n",
              "      <td>-1.442</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.836</td>\n",
              "      <td>-0.326</td>\n",
              "      <td>0.716</td>\n",
              "      <td>-0.764</td>\n",
              "      <td>0.248</td>\n",
              "      <td>-1.308</td>\n",
              "      <td>2.127</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.296</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>1.854</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.999</td>\n",
              "      <td>-1.171</td>\n",
              "      <td>2.798</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-1.048</td>\n",
              "      <td>1.078</td>\n",
              "      <td>0.401</td>\n",
              "      <td>-0.486</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.083</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>1.251</td>\n",
              "      <td>-0.206</td>\n",
              "      <td>-0.933</td>\n",
              "      <td>-1.215</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.512</td>\n",
              "      <td>-0.424</td>\n",
              "      <td>0.769</td>\n",
              "      <td>0.223</td>\n",
              "      <td>-0.710</td>\n",
              "      <td>2.725</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.845</td>\n",
              "      <td>-1.226</td>\n",
              "      <td>1.527</td>\n",
              "      <td>-1.701</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.150</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.322</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>1.282</td>\n",
              "      <td>0.408</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>1.020</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>-1.574</td>\n",
              "      <td>-1.618</td>\n",
              "      <td>-0.404</td>\n",
              "      <td>0.640</td>\n",
              "      <td>-0.595</td>\n",
              "      <td>-0.966</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-0.562</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>0.238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.347</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>0.511</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>1.225</td>\n",
              "      <td>1.594</td>\n",
              "      <td>0.585</td>\n",
              "      <td>1.509</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>2.198</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.453</td>\n",
              "      <td>0.494</td>\n",
              "      <td>1.478</td>\n",
              "      <td>-1.412</td>\n",
              "      <td>0.270</td>\n",
              "      <td>-1.312</td>\n",
              "      <td>-0.322</td>\n",
              "      <td>-0.688</td>\n",
              "      <td>-0.198</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>1.042</td>\n",
              "      <td>-0.315</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>0.024</td>\n",
              "      <td>-0.190</td>\n",
              "      <td>1.656</td>\n",
              "      <td>-0.469</td>\n",
              "      <td>-1.437</td>\n",
              "      <td>-0.581</td>\n",
              "      <td>-0.308</td>\n",
              "      <td>-0.837</td>\n",
              "      <td>-1.739</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.336</td>\n",
              "      <td>-1.102</td>\n",
              "      <td>2.371</td>\n",
              "      <td>0.554</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.050</td>\n",
              "      <td>-0.347</td>\n",
              "      <td>0.904</td>\n",
              "      <td>-1.324</td>\n",
              "      <td>-0.849</td>\n",
              "      <td>3.432</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.174</td>\n",
              "      <td>-1.517</td>\n",
              "      <td>-0.337</td>\n",
              "      <td>0.055</td>\n",
              "      <td>-0.464</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-1.073</td>\n",
              "      <td>0.325</td>\n",
              "      <td>-0.523</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>0.190</td>\n",
              "      <td>-0.883</td>\n",
              "      <td>-1.830</td>\n",
              "      <td>1.408</td>\n",
              "      <td>2.319</td>\n",
              "      <td>1.704</td>\n",
              "      <td>-0.723</td>\n",
              "      <td>1.014</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.096</td>\n",
              "      <td>-0.775</td>\n",
              "      <td>1.845</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.134</td>\n",
              "      <td>2.415</td>\n",
              "      <td>-0.996</td>\n",
              "      <td>-1.006</td>\n",
              "      <td>1.378</td>\n",
              "      <td>1.246</td>\n",
              "      <td>1.478</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.253</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 302 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  target      0      1      2  ...    295    296    297    298    299\n",
              "0   0     1.0 -0.098  2.165  0.681  ... -2.097  1.051 -0.414  1.038 -1.065\n",
              "1   1     0.0  1.081 -0.973 -0.383  ... -1.624 -0.458 -1.099 -0.936  0.973\n",
              "2   2     1.0 -0.523 -0.089 -0.348  ... -1.165 -1.544  0.004  0.800 -1.211\n",
              "3   3     1.0  0.067 -0.021  0.392  ...  0.467 -0.562 -0.254 -0.533  0.238\n",
              "4   4     1.0  2.347 -0.831  0.511  ...  1.378  1.246  1.478  0.428  0.253\n",
              "\n",
              "[5 rows x 302 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShuYAMsyVTue",
        "colab_type": "text"
      },
      "source": [
        "Prepare the data sets for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-7iWdWsXmKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare for modeling\n",
        "X_train = train.drop(['id', 'target'], axis=1)\n",
        "y_train = train['target']\n",
        "\n",
        "X_test = test.drop(['id'], axis=1)\n",
        "\n",
        "# scaling data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Oh_WzTYVTzW",
        "colab_type": "text"
      },
      "source": [
        "## Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIyKp8W9XocV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define models\n",
        "ridge = linear_model.Ridge()\n",
        "lasso = linear_model.Lasso()\n",
        "elastic = linear_model.ElasticNet()\n",
        "lasso_lars = linear_model.LassoLars()\n",
        "bayesian_ridge = linear_model.BayesianRidge()\n",
        "logistic = linear_model.LogisticRegression(solver='liblinear')\n",
        "sgd = linear_model.SGDClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIciWM6eXtJd",
        "colab_type": "text"
      },
      "source": [
        "Here we select seven common traditional machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8L8OslwUMru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [ridge, lasso, elastic, lasso_lars, bayesian_ridge, logistic, sgd]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EzvLtYVXxI2",
        "colab_type": "text"
      },
      "source": [
        "### Get Basic Metrics\n",
        "\n",
        "We then find the mean cross validation score and standard deviation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_gWxEKeXx52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to get cross validation scores\n",
        "def get_cv_scores(model):\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "    print('CV Mean: ', np.mean(scores))\n",
        "    print('STD: ', np.std(scores))\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mApTAseoX2PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loop through list of models\n",
        "for model in models:\n",
        "    print(model)\n",
        "    get_cv_scores(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwhV28zvX-LV",
        "colab_type": "text"
      },
      "source": [
        "From this we can see our best performing models out of the box are logistic regression and stochastic gradient descent. Let's see if we can optimize these models with hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV-jZ6MmX-S7",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression and Grid Search\n",
        "\n",
        "Grid search is a traditional way to perform hyperparameter optimization. It works by searching exhaustively through a specified subset of hyperparameters.\n",
        "Using sklearn’s **[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)**, we first define our grid of parameters to search over and then run the grid search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuyDDYSdYEIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "penalty = ['l1', 'l2']\n",
        "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
        "solver = ['liblinear', 'saga']\n",
        "\n",
        "param_grid = dict(penalty=penalty,\n",
        "                  C=C,\n",
        "                  class_weight=class_weight,\n",
        "                  solver=solver)\n",
        "\n",
        "grid = GridSearchCV(estimator=logistic, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "print('Best Score: ', grid_result.best_score_)\n",
        "print('Best Params: ', grid_result.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fws--KGDYIQ_",
        "colab_type": "text"
      },
      "source": [
        "We improved our cross validation score from 0.744 to 0.789!\n",
        "\n",
        "The benefit of grid search is that it is guaranteed to find the optimal combination of parameters supplied. The drawback is that it can be very time consuming and computationally expensive.\n",
        "\n",
        "We can combat this with random search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DR63HsrYMGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logistic = linear_model.LogisticRegression(C=1, class_weight={1:0.6, 0:0.4}, penalty='l1', solver='liblinear')\n",
        "get_cv_scores(logistic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zuyhzbi3YPfP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Eu_FAoYP20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = logistic.fit(X_train, y_train).predict_proba(X_test)\n",
        "#### score 0.828 on public leaderboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a3IaslNYTbd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYIOgdTGYToR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv('../input/sample_submission.csv')\n",
        "submission['target'] = predictions\n",
        "#submission.to_csv('submission.csv', index=False)\n",
        "submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_5QHIw6YVf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRxdH-gKYYv3",
        "colab_type": "text"
      },
      "source": [
        "## Stochastic Gradient Descent and Random Search\n",
        "Random search is a random (obviously) search over specified parameter values.\n",
        "\n",
        "### Random Search\n",
        "\n",
        "Random search differs from grid search mainly in that it searches the specified subset of hyperparameters randomly instead of exhaustively. The major benefit being decreased processing time.\n",
        "\n",
        "There is a tradeoff to decreased processing time, however. We aren’t guaranteed to find the optimal combination of hyperparameters.\n",
        "\n",
        "Let’s give random search a try with sklearn’s **[RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)**. Very similar to grid search above, we define the hyperparameters to search over before running the search.\n",
        "An important additional parameter to specify here is n_iter. This specifies the number of combinations to randomly try.\n",
        "\n",
        "- Selecting too low of a number will decrease our chance of finding the best combination. \n",
        "\n",
        "- Selecting too large of a number will increase our processing time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjqSs6ghYam4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']\n",
        "penalty = ['l1', 'l2', 'elasticnet']\n",
        "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
        "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
        "eta0 = [1, 10, 100]\n",
        "\n",
        "param_distributions = dict(loss=loss,\n",
        "                           penalty=penalty,\n",
        "                           alpha=alpha,\n",
        "                           learning_rate=learning_rate,\n",
        "                           class_weight=class_weight,\n",
        "                           eta0=eta0)\n",
        "\n",
        "random = RandomizedSearchCV(estimator=sgd, param_distributions=param_distributions, scoring='roc_auc', verbose=1, n_jobs=-1, n_iter=1000)\n",
        "random_result = random.fit(X_train, y_train)\n",
        "\n",
        "print('Best Score: ', random_result.best_score_)\n",
        "print('Best Params: ', random_result.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDKO36KBYgVZ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQbz9-1xYj4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = linear_model.SGDClassifier(alpha=0.1,\n",
        "                                 class_weight={1:0.7, 0:0.3},\n",
        "                                 eta0=100,\n",
        "                                 learning_rate='optimal',\n",
        "                                 loss='log',\n",
        "                                 penalty='elasticnet')\n",
        "get_cv_scores(sgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2vGBnw7Ymq7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXwk625VYm0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = sgd.fit(X_train, y_train).predict_proba(X_test)\n",
        "#### score 0.790 on public leaderboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIbpaVYkYnss",
        "colab_type": "text"
      },
      "source": [
        "Here we improved the cross validation score from 0.733 to 0.780!\n",
        "Conclusion\n",
        "Here we explored two methods for hyperparameter turning and saw improvement in model performance.\n",
        "While this is an important step in modeling, it is by no means the only way to improve performance.\n",
        "In future articles we will explore other means to prevent overfitting including feature selection and ensembling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1jfSWkPbeMJ",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Here we explored two methods for hyperparameter turning and saw improvement in model performance.\n",
        "\n",
        "While this is an important step in modeling, it is by no means the only way to improve performance.\n",
        "\n",
        "In future articles we will explore other means to prevent overfitting including feature selection and ensembling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E61FtcVxYn0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv('../input/sample_submission.csv')\n",
        "submission['target'] = predictions\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVOAS8V0Yqi3",
        "colab_type": "text"
      },
      "source": [
        "So since this is used in a Kaggle Competition, the results are saved into a submission CSV dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpUlhl-LYvN3",
        "colab_type": "text"
      },
      "source": [
        "### End of notebook."
      ]
    }
  ]
}